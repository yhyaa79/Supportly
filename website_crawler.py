from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import time
from collections import deque
import re

import os
from urllib.parse import urlparse

def setup_output_folder(base_url):
    """Ø³Ø§Ø®Øª ÙÙˆÙ„Ø¯Ø± Ø§ØµÙ„ÛŒ + ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ Ø³Ø§ÛŒØª"""
    # --- ÙÙˆÙ„Ø¯Ø± Ø§ØµÙ„ÛŒ ---
    main_folder = "advanced_website_crawler"
    os.makedirs(main_folder, exist_ok=True)
    
    # --- Ù†Ø§Ù… Ø¯Ø§Ù…Ù†Ù‡ ---
    domain = urlparse(base_url).netloc
    folder_name = domain.replace('www.', '')
    folder_path = os.path.join(main_folder, folder_name)
    
    if os.path.exists(folder_path):
        print(f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÛŒÙ† Ø³Ø§ÛŒØª Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø³Øª!")
        print(f"Ù…Ø³ÛŒØ±: {folder_path}")
        print("Ù‡ÛŒÚ† Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯.")
        return None
    
    os.makedirs(folder_path, exist_ok=True)
    print(f"ÙÙˆÙ„Ø¯Ø± Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯:")
    print(f"   {folder_path}")
    return folder_path


class AdvancedWebsiteCrawler:
    def __init__(self, base_url, max_pages=100, headless=True):
        self.base_url = base_url
        self.max_pages = max_pages
        self.visited_urls = set()
        self.pages_data = []
        self.domain = urlparse(base_url).netloc
        
        # Ø³Ø§Ø®Øª ÙÙˆÙ„Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ
        self.output_folder = setup_output_folder(base_url)
        if not self.output_folder:
            raise SystemExit("Ø®Ø²ÛŒØ¯Ù† Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Selenium
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def is_valid_url(self, url):
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù† URL"""
        if not url or url.startswith('#') or url.startswith('javascript:'):
            return False
            
        parsed = urlparse(url)
        return (parsed.netloc == self.domain and 
                parsed.scheme in ['http', 'https'] and
                not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.mp4', '.css', '.js']))
    
    def scroll_page(self):
        """Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ lazy-load"""
        try:
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            for _ in range(3):  # 3 Ø¨Ø§Ø± Ø§Ø³Ú©Ø±ÙˆÙ„
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÙˆÙ„: {e}")
    
    def extract_page_content(self, url):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© ØµÙØ­Ù‡ Ø¨Ø§ Selenium"""
        try:
            print(f"   â†’ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡...")
            self.driver.get(url)
            
            # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡
            time.sleep(3)
            
            # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ©
            self.scroll_page()
            
            # Ø¯Ø±ÛŒØ§ÙØª HTML Ú©Ø§Ù…Ù„ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ JavaScript
            page_source = self.driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
            for tag in soup(['script', 'style', 'noscript']):
                tag.decompose()
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
            title = self.driver.title or ''
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§ ØªÙˆØ¶ÛŒØ­Ø§Øª
            meta_desc = ''
            try:
                meta_element = self.driver.find_element(By.CSS_SELECTOR, 'meta[name="description"]')
                meta_desc = meta_element.get_attribute('content') or ''
            except:
                pass
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… Ù…ØªÙ† Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡
            try:
                body = self.driver.find_element(By.TAG_NAME, 'body')
                text_content = body.text
                text_content = re.sub(r'\s+', ' ', text_content).strip()
            except:
                text_content = soup.get_text(separator=' ', strip=True)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§
            headings = []
            for i in range(1, 7):
                try:
                    heading_elements = self.driver.find_elements(By.TAG_NAME, f'h{i}')
                    for heading in heading_elements:
                        h_text = heading.text.strip()
                        if h_text:
                            headings.append(h_text)
                except:
                    pass
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
            links = []
            try:
                link_elements = self.driver.find_elements(By.TAG_NAME, 'a')
                for link in link_elements:
                    try:
                        href = link.get_attribute('href')
                        if href:
                            absolute_url = urljoin(url, href)
                            # Ø­Ø°Ù fragment Ø§Ø² URL
                            absolute_url = absolute_url.split('#')[0]
                            if self.is_valid_url(absolute_url):
                                links.append(absolute_url)
                    except:
                        continue
            except:
                pass
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ
            page_data = {
                'url': url,
                'title': title,
                'meta_description': meta_desc,
                'headings': headings,
                'content': text_content[:8000],  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§
                'content_length': len(text_content),
                'links': list(set(links)),
                'links_count': len(set(links))
            }
            
            print(f"   âœ“ Ø¹Ù†ÙˆØ§Ù†: {title[:50]}...")
            print(f"   âœ“ Ù…Ø­ØªÙˆØ§: {len(text_content)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
            print(f"   âœ“ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {len(set(links))} Ø¹Ø¯Ø¯")
            
            return page_data
            
        except Exception as e:
            print(f"   âœ— Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ {url}: {str(e)}")
            return None
    
    def crawl(self):
        """Ø®Ø²ÛŒØ¯Ù† Ø±ÙˆÛŒ ÙˆØ¨Ø³Ø§ÛŒØª"""
        queue = deque([self.base_url])
        self.visited_urls.add(self.base_url)
        
        print("="*70)
        print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø®Ø²ÛŒØ¯Ù† Ø§Ø²: {self.base_url}")
        print(f"ğŸ“Š Ø­Ø¯Ø§Ú©Ø«Ø± ØµÙØ­Ø§Øª: {self.max_pages}")
        print("="*70 + "\n")
        
        try:
            while queue and len(self.pages_data) < self.max_pages:
                current_url = queue.popleft()
                print(f"\nğŸ“„ [{len(self.pages_data) + 1}/{self.max_pages}] {current_url}")
                
                page_data = self.extract_page_content(current_url)
                
                if page_data:
                    self.pages_data.append(page_data)
                    
                    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ
                    new_links = 0
                    for link in page_data['links']:
                        if link not in self.visited_urls and len(self.visited_urls) < self.max_pages * 2:
                            self.visited_urls.add(link)
                            queue.append(link)
                            new_links += 1
                    
                    print(f"   âœ“ {new_links} Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
                
                time.sleep(2)  # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
            
            print("\n" + "="*70)
            print(f"âœ… Ø®Ø²ÛŒØ¯Ù† ØªÙ…Ø§Ù… Ø´Ø¯!")
            print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(self.pages_data)}")
            print(f"ğŸ”— ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ URL Ù‡Ø§ÛŒ Ú©Ø´Ù Ø´Ø¯Ù‡: {len(self.visited_urls)}")
            print("="*70 + "\n")
            
        finally:
            self.driver.quit()
        
        return self.pages_data
    
    def save_to_file(self, filename='website_data.json'):
        """Ø°Ø®ÛŒØ±Ù‡ JSON Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ Ø³Ø§ÛŒØª"""
        filepath = os.path.join(self.output_folder, filename)
        data = {
            'base_url': self.base_url,
            'total_pages': len(self.pages_data),
            'total_urls_discovered': len(self.visited_urls),
            'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'pages': self.pages_data
        }
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:\n {filepath}")
        
    
    def create_sitemap(self, filename='sitemap.txt'):
        """Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª"""
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"# Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª: {self.base_url}\n")
            f.write(f"# ØªØ§Ø±ÛŒØ®: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"# ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(self.pages_data)}\n\n")
            
            for i, page in enumerate(self.pages_data, 1):
                f.write(f"{i}. {page['url']}\n")
                f.write(f"   Ø¹Ù†ÙˆØ§Ù†: {page['title']}\n")
                f.write(f"   ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©: {page['links_count']}\n\n")
        
        print(f"ğŸ—ºï¸  Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª Ø¯Ø± ÙØ§ÛŒÙ„ {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def create_summary(self, filename='summary.txt'):
            filepath = os.path.join(self.output_folder, filename)
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write("=" * 70 + "\n")
                f.write(f"Ú¯Ø²Ø§Ø±Ø´ Ø®Ø²ÛŒØ¯Ù† ÙˆØ¨Ø³Ø§ÛŒØª\n")
                f.write("=" * 70 + "\n\n")
                f.write(f"Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡: {self.base_url}\n")
                f.write(f"ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(self.pages_data)}\n")
                f.write(f"ØªØ¹Ø¯Ø§Ø¯ URL Ù‡Ø§ÛŒ Ú©Ø´Ù Ø´Ø¯Ù‡: {len(self.visited_urls)}\n")
                f.write(f"ØªØ§Ø±ÛŒØ® Ø®Ø²ÛŒØ¯Ù†: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                f.write("-" * 70 + "\n")
                f.write("ÙÙ‡Ø±Ø³Øª ØµÙØ­Ø§Øª:\n")
                f.write("-" * 70 + "\n\n")
                
                for i, page in enumerate(self.pages_data, 1):
                    f.write(f"{i}. {page['title']}\n")
                    f.write(f"   URL: {page['url']}\n")
                    f.write(f"   Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§: {page['content_length']} Ú©Ø§Ø±Ø§Ú©ØªØ±\n")
                    f.write(f"   ØªØ¹Ø¯Ø§Ø¯ Ù‡Ø¯ÛŒÙ†Ú¯: {len(page['headings'])}\n")
                    f.write(f"   ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©: {page['links_count']}\n\n")
            
            print(f"ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:\n   {filepath}")



def start_website_crawler(website_url):
    
    print("\n" + "="*70)
    print("Ú©Ø±Ø§Ù„Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ÙˆØ¨Ø³Ø§ÛŒØª Ø¨Ø§ Selenium")
    print("Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ + Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±")
    print("="*70 + "\n")
    
    try:
        crawler = AdvancedWebsiteCrawler(
            website_url, 
            max_pages=50,
            headless=True
        )
        
        # Ø´Ø±ÙˆØ¹ Ø®Ø²ÛŒØ¯Ù†
        crawler.crawl()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø³Ø§ÛŒØª
        crawler.save_to_file('website_data.json')
        crawler.create_sitemap('sitemap.txt')
        crawler.create_summary('summary.txt')
        
        print(f"\nğŸ‰ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø²ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
        print(f"   ğŸ“‚ {crawler.output_folder}")
        
    except SystemExit:
        pass  # Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø¨ÙˆØ¯ØŒ ÙÙ‚Ø· Ù¾ÛŒØ§Ù… Ø¯Ø§Ø¯Ù‡ Ùˆ Ø®Ø§Ø±Ø¬ Ø´Ø¯Ù‡
    except Exception as e:
        print(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")