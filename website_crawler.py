from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import json
import time
from collections import deque
import re
import os

def setup_output_folder(base_url):
    """Ø³Ø§Ø®Øª ÙÙˆÙ„Ø¯Ø± Ø§ØµÙ„ÛŒ + ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ Ø³Ø§ÛŒØª"""
    # --- ÙÙˆÙ„Ø¯Ø± Ø§ØµÙ„ÛŒ ---
    main_folder = "advanced_website_crawler"
    os.makedirs(main_folder, exist_ok=True)
    
    # --- Ù†Ø§Ù… Ø¯Ø§Ù…Ù†Ù‡ ---
    domain = urlparse(base_url).netloc
    folder_name = domain.replace('www.', '')
    folder_path = os.path.join(main_folder, folder_name)
    
    if os.path.exists(folder_path):
        print(f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§ÛŒÙ† Ø³Ø§ÛŒØª Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø³Øª!")
        print(f"Ù…Ø³ÛŒØ±: {folder_path}")
        print("Ù‡ÛŒÚ† Ø¹Ù…Ù„ÛŒØ§ØªÛŒ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯.")
        return None
    
    os.makedirs(folder_path, exist_ok=True)
    print(f"ÙÙˆÙ„Ø¯Ø± Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯: {folder_path}")
    return folder_path

def is_valid_url(url, domain):
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù† URL"""
    if not url or url.startswith('#') or url.startswith('javascript:'):
        return False
    parsed = urlparse(url)
    return (parsed.netloc == domain and
            parsed.scheme in ['http', 'https'] and
            not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.mp4', '.css', '.js']))

def scroll_page(driver):
    """Ø§Ø³Ú©Ø±ÙˆÙ„ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ lazy-load"""
    try:
        last_height = driver.execute_script("return document.body.scrollHeight")
        for _ in range(3):  # 3 Ø¨Ø§Ø± Ø§Ø³Ú©Ø±ÙˆÙ„
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                break
            last_height = new_height
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÙˆÙ„: {e}")

def extract_page_content(driver, url, domain):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© ØµÙØ­Ù‡ Ø¨Ø§ Selenium"""
    try:
        print(f" â†’ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡...")
        driver.get(url)
        # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø§Ù…Ù„ ØµÙØ­Ù‡
        time.sleep(3)
        # Ø§Ø³Ú©Ø±ÙˆÙ„ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ©
        scroll_page(driver)
        # Ø¯Ø±ÛŒØ§ÙØª HTML Ú©Ø§Ù…Ù„ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ JavaScript
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
        for tag in soup(['script', 'style', 'noscript']):
            tag.decompose()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        title = driver.title or ''
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§ ØªÙˆØ¶ÛŒØ­Ø§Øª
        meta_desc = ''
        try:
            meta_element = driver.find_element(By.CSS_SELECTOR, 'meta[name="description"]')
            meta_desc = meta_element.get_attribute('content') or ''
        except:
            pass
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… Ù…ØªÙ† Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡
        try:
            body = driver.find_element(By.TAG_NAME, 'body')
            text_content = body.text
            text_content = re.sub(r'\s+', ' ', text_content).strip()
        except:
            text_content = soup.get_text(separator=' ', strip=True)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§
        headings = []
        for i in range(1, 7):
            try:
                heading_elements = driver.find_elements(By.TAG_NAME, f'h{i}')
                for heading in heading_elements:
                    h_text = heading.text.strip()
                    if h_text:
                        headings.append(h_text)
            except:
                pass
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        links = []
        try:
            link_elements = driver.find_elements(By.TAG_NAME, 'a')
            for link in link_elements:
                try:
                    href = link.get_attribute('href')
                    if href:
                        absolute_url = urljoin(url, href)
                        # Ø­Ø°Ù fragment Ø§Ø² URL
                        absolute_url = absolute_url.split('#')[0]
                        if is_valid_url(absolute_url, domain):
                            links.append(absolute_url)
                except:
                    continue
        except:
            pass
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø¶Ø§ÙÛŒ
        page_data = {
            'url': url,
            'title': title,
            'meta_description': meta_desc,
            'headings': headings,
            'content': text_content[:8000],  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§
            'content_length': len(text_content),
            'links': list(set(links)),
            'links_count': len(set(links))
        }
        print(f" âœ“ Ø¹Ù†ÙˆØ§Ù†: {title[:50]}...")
        print(f" âœ“ Ù…Ø­ØªÙˆØ§: {len(text_content)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
        print(f" âœ“ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {len(set(links))} Ø¹Ø¯Ø¯")
        return page_data
    except Exception as e:
        print(f" âœ— Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ {url}: {str(e)}")
        return None

def crawl_website(base_url, max_pages=50):
    """Ø®Ø²ÛŒØ¯Ù† Ø±ÙˆÛŒ ÙˆØ¨Ø³Ø§ÛŒØª (Ø¨Ø¯ÙˆÙ† Ú©Ù„Ø§Ø³)"""
    domain = urlparse(base_url).netloc
    output_folder = setup_output_folder(base_url)
    if not output_folder:
        return None
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Selenium
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
    
    driver = webdriver.Chrome(options=chrome_options)
    wait = WebDriverWait(driver, 10)
    
    visited_urls = set()
    pages_data = []
    queue = deque([base_url])
    visited_urls.add(base_url)
    
    print("="*70)
    print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø®Ø²ÛŒØ¯Ù† Ø§Ø²: {base_url}")
    print(f"ğŸ“Š Ø­Ø¯Ø§Ú©Ø«Ø± ØµÙØ­Ø§Øª: {max_pages}")
    print("="*70 + "\n")
    
    try:
        while queue and len(pages_data) < max_pages:
            current_url = queue.popleft()
            print(f"\nğŸ“„ [{len(pages_data) + 1}/{max_pages}] {current_url}")
            page_data = extract_page_content(driver, current_url, domain)
            if page_data:
                pages_data.append(page_data)
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ
                new_links = 0
                for link in page_data['links']:
                    if link not in visited_urls and len(visited_urls) < max_pages * 2:
                        visited_urls.add(link)
                        queue.append(link)
                        new_links += 1
                print(f" âœ“ {new_links} Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
            time.sleep(2)  # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
        
        print("\n" + "="*70)
        print(f"âœ… Ø®Ø²ÛŒØ¯Ù† ØªÙ…Ø§Ù… Ø´Ø¯!")
        print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(pages_data)}")
        print(f"ğŸ”— ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ URL Ù‡Ø§ÛŒ Ú©Ø´Ù Ø´Ø¯Ù‡: {len(visited_urls)}")
        print("="*70 + "\n")
        
        return pages_data, visited_urls, output_folder, driver
    finally:
        driver.quit()

def save_to_file(pages_data, base_url, visited_urls, output_folder, filename='website_data.json'):
    """Ø°Ø®ÛŒØ±Ù‡ JSON Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ Ø³Ø§ÛŒØª"""
    filepath = os.path.join(output_folder, filename)
    data = {
        'base_url': base_url,
        'total_pages': len(pages_data),
        'total_urls_discovered': len(visited_urls),
        'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S'),
        'pages': pages_data
    }
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filepath}")

def create_sitemap(pages_data, base_url, output_folder, filename='sitemap.txt'):
    """Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª"""
    filepath = os.path.join(output_folder, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(f"# Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª: {base_url}\n")
        f.write(f"# ØªØ§Ø±ÛŒØ®: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(pages_data)}\n\n")
        for i, page in enumerate(pages_data, 1):
            f.write(f"{i}. {page['url']}\n")
            f.write(f" Ø¹Ù†ÙˆØ§Ù†: {page['title']}\n")
            f.write(f" ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©: {page['links_count']}\n\n")
    print(f"ğŸ—ºï¸ Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª Ø¯Ø± ÙØ§ÛŒÙ„ {filepath} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

def create_summary(pages_data, base_url, visited_urls, output_folder, filename='summary.txt'):
    """Ø§ÛŒØ¬Ø§Ø¯ Ø®Ù„Ø§ØµÙ‡"""
    filepath = os.path.join(output_folder, filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write(f"Ú¯Ø²Ø§Ø±Ø´ Ø®Ø²ÛŒØ¯Ù† ÙˆØ¨Ø³Ø§ÛŒØª\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"Ø¢Ø¯Ø±Ø³ Ù¾Ø§ÛŒÙ‡: {base_url}\n")
        f.write(f"ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(pages_data)}\n")
        f.write(f"ØªØ¹Ø¯Ø§Ø¯ URL Ù‡Ø§ÛŒ Ú©Ø´Ù Ø´Ø¯Ù‡: {len(visited_urls)}\n")
        f.write(f"ØªØ§Ø±ÛŒØ® Ø®Ø²ÛŒØ¯Ù†: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("-" * 70 + "\n")
        f.write("ÙÙ‡Ø±Ø³Øª ØµÙØ­Ø§Øª:\n")
        f.write("-" * 70 + "\n\n")
        for i, page in enumerate(pages_data, 1):
            f.write(f"{i}. {page['title']}\n")
            f.write(f" URL: {page['url']}\n")
            f.write(f" Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§: {page['content_length']} Ú©Ø§Ø±Ø§Ú©ØªØ±\n")
            f.write(f" ØªØ¹Ø¯Ø§Ø¯ Ù‡Ø¯ÛŒÙ†Ú¯: {len(page['headings'])}\n")
            f.write(f" ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©: {page['links_count']}\n\n")
    print(f"ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {filepath}")

def start_website_crawler(website_url, max_pages=50):
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø±ÙˆØ¹ Ú©Ø±Ø§Ù„Ø± (Ø³Ø§Ø¯Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø¯ÙˆÙ† Ú©Ù„Ø§Ø³)"""
    print("\n" + "="*70)
    print("Ú©Ø±Ø§Ù„Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ÙˆØ¨Ø³Ø§ÛŒØª Ø¨Ø§ Selenium")
    print("Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ + Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±")
    print("="*70 + "\n")
    
    try:
        result = crawl_website(website_url, max_pages)
        if not result:
            return
        
        pages_data, visited_urls, output_folder, _ = result
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù‡Ù…Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø³Ø§ÛŒØª
        save_to_file(pages_data, website_url, visited_urls, output_folder, 'website_data.json')
        create_sitemap(pages_data, website_url, output_folder, 'sitemap.txt')
        create_summary(pages_data, website_url, visited_urls, output_folder, 'summary.txt')
        
        print(f"\nğŸ‰ Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø²ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
        print(f" ğŸ“‚ {output_folder}")
        
    except Exception as e:
        print(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")

# Ù…Ø«Ø§Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡:
# start_website_crawler("https://example.com", max_pages=50)