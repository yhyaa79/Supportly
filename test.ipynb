{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d166aa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Ú©Ø±Ø§Ù„Ø± Ù¾ÛŒØ´Ø±ÙØªÙ‡ ÙˆØ¨Ø³Ø§ÛŒØª Ø¨Ø§ Selenium\n",
      "Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ + Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±\n",
      "======================================================================\n",
      "\n",
      "ÙÙˆÙ„Ø¯Ø± Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯:\n",
      "   advanced_website_crawler/console.melipayamak.com\n",
      "======================================================================\n",
      "ğŸš€ Ø´Ø±ÙˆØ¹ Ø®Ø²ÛŒØ¯Ù† Ø§Ø²: https://console.melipayamak.com/\n",
      "ğŸ“Š Ø­Ø¯Ø§Ú©Ø«Ø± ØµÙØ­Ø§Øª: 50\n",
      "======================================================================\n",
      "\n",
      "\n",
      "ğŸ“„ [1/50] https://console.melipayamak.com/\n",
      "   â†’ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡...\n",
      "   âœ“ Ø¹Ù†ÙˆØ§Ù†: Ú©Ù†Ø³ÙˆÙ„ Ù…Ù„ÛŒ Ù¾ÛŒØ§Ù…Ú©...\n",
      "   âœ“ Ù…Ø­ØªÙˆØ§: 222 Ú©Ø§Ø±Ø§Ú©ØªØ±\n",
      "   âœ“ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: 2 Ø¹Ø¯Ø¯\n",
      "   âœ“ 2 Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯\n",
      "\n",
      "ğŸ“„ [2/50] https://console.melipayamak.com/auth/register\n",
      "   â†’ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡...\n",
      "   âœ“ Ø¹Ù†ÙˆØ§Ù†: Ú©Ù†Ø³ÙˆÙ„ Ù…Ù„ÛŒ Ù¾ÛŒØ§Ù…Ú©...\n",
      "   âœ“ Ù…Ø­ØªÙˆØ§: 244 Ú©Ø§Ø±Ø§Ú©ØªØ±\n",
      "   âœ“ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: 1 Ø¹Ø¯Ø¯\n",
      "   âœ“ 1 Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯\n",
      "\n",
      "ğŸ“„ [3/50] https://console.melipayamak.com/auth/forgotPassword\n",
      "   â†’ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡...\n",
      "   âœ“ Ø¹Ù†ÙˆØ§Ù†: Ú©Ù†Ø³ÙˆÙ„ Ù…Ù„ÛŒ Ù¾ÛŒØ§Ù…Ú©...\n",
      "   âœ“ Ù…Ø­ØªÙˆØ§: 277 Ú©Ø§Ø±Ø§Ú©ØªØ±\n",
      "   âœ“ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: 1 Ø¹Ø¯Ø¯\n",
      "   âœ“ 0 Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯\n",
      "\n",
      "ğŸ“„ [4/50] https://console.melipayamak.com/auth/login\n",
      "   â†’ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØµÙØ­Ù‡...\n",
      "   âœ— Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ https://console.melipayamak.com/auth/login: Message: unknown error: net::ERR_CONNECTION_RESET\n",
      "  (Session info: chrome=141.0.7390.123)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000103263cf8 cxxbridge1$str$ptr + 2895872\n",
      "1   chromedriver                        0x000000010325bc34 cxxbridge1$str$ptr + 2862908\n",
      "2   chromedriver                        0x0000000102d81570 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74324\n",
      "3   chromedriver                        0x0000000102d79634 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 41752\n",
      "4   chromedriver                        0x0000000102d6c1a4 cxxbridge1$string$len + 4708\n",
      "5   chromedriver                        0x0000000102d6dc3c cxxbridge1$string$len + 11516\n",
      "6   chromedriver                        0x0000000102d6c61c cxxbridge1$string$len + 5852\n",
      "7   chromedriver                        0x0000000102d6bf0c cxxbridge1$string$len + 4044\n",
      "8   chromedriver                        0x0000000102d6bc98 cxxbridge1$string$len + 3416\n",
      "9   chromedriver                        0x0000000102d69aa0 chromedriver + 203424\n",
      "10  chromedriver                        0x0000000102d6a4f8 chromedriver + 206072\n",
      "11  chromedriver                        0x0000000102d845e4 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 86728\n",
      "12  chromedriver                        0x0000000102e0ae90 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 637812\n",
      "13  chromedriver                        0x0000000102e0a3d8 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 635068\n",
      "14  chromedriver                        0x0000000102dbd0f8 _RNvCs47EqcsrPRmA_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 318940\n",
      "15  chromedriver                        0x000000010322781c cxxbridge1$str$ptr + 2648868\n",
      "16  chromedriver                        0x000000010322adf8 cxxbridge1$str$ptr + 2662656\n",
      "17  chromedriver                        0x0000000103208334 cxxbridge1$str$ptr + 2520636\n",
      "18  chromedriver                        0x000000010322b6e0 cxxbridge1$str$ptr + 2664936\n",
      "19  chromedriver                        0x00000001031f9a80 cxxbridge1$str$ptr + 2461064\n",
      "20  chromedriver                        0x000000010324b014 cxxbridge1$str$ptr + 2794268\n",
      "21  chromedriver                        0x000000010324b198 cxxbridge1$str$ptr + 2794656\n",
      "22  chromedriver                        0x000000010325b880 cxxbridge1$str$ptr + 2861960\n",
      "23  libsystem_pthread.dylib             0x00000001959942e4 _pthread_start + 136\n",
      "24  libsystem_pthread.dylib             0x000000019598f0fc thread_start + 8\n",
      "\n",
      "\n",
      "======================================================================\n",
      "âœ… Ø®Ø²ÛŒØ¯Ù† ØªÙ…Ø§Ù… Ø´Ø¯!\n",
      "ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: 3\n",
      "ğŸ”— ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ URL Ù‡Ø§ÛŒ Ú©Ø´Ù Ø´Ø¯Ù‡: 4\n",
      "======================================================================\n",
      "\n",
      "Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: AdvancedWebsiteCrawler.save_to_file() takes from 1 to 2 positional arguments but 3 were given\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import json\n",
    "import time\n",
    "from collections import deque\n",
    "import re\n",
    "\n",
    "class WebsiteCrawler:\n",
    "    def __init__(self, base_url, max_pages=100):\n",
    "        self.base_url = base_url\n",
    "        self.max_pages = max_pages\n",
    "        self.visited_urls = set()\n",
    "        self.pages_data = []\n",
    "        self.domain = urlparse(base_url).netloc\n",
    "        \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨ÙˆØ¯Ù† URL\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        return (parsed.netloc == self.domain and \n",
    "                parsed.scheme in ['http', 'https'] and\n",
    "                not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.zip', '.mp4']))\n",
    "    \n",
    "    def extract_page_content(self, url):\n",
    "        \"\"\"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ ÛŒÚ© ØµÙØ­Ù‡\"\"\"\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ\n",
    "            for tag in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                tag.decompose()\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª\n",
    "            title = soup.find('title').get_text() if soup.find('title') else ''\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªØ§ ØªÙˆØ¶ÛŒØ­Ø§Øª\n",
    "            meta_desc = ''\n",
    "            meta_tag = soup.find('meta', attrs={'name': 'description'})\n",
    "            if meta_tag and meta_tag.get('content'):\n",
    "                meta_desc = meta_tag['content']\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… Ù…ØªÙ† ØµÙØ­Ù‡\n",
    "            text_content = soup.get_text(separator=' ', strip=True)\n",
    "            text_content = re.sub(r'\\s+', ' ', text_content)\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§\n",
    "            headings = []\n",
    "            for i in range(1, 7):\n",
    "                for heading in soup.find_all(f'h{i}'):\n",
    "                    headings.append(heading.get_text(strip=True))\n",
    "            \n",
    "            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_url = urljoin(url, link['href'])\n",
    "                if self.is_valid_url(absolute_url):\n",
    "                    links.append(absolute_url)\n",
    "            \n",
    "            return {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'meta_description': meta_desc,\n",
    "                'headings': headings,\n",
    "                'content': text_content[:5000],  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ø­ØªÙˆØ§\n",
    "                'links': list(set(links))\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ {url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Ø®Ø²ÛŒØ¯Ù† Ø±ÙˆÛŒ ÙˆØ¨Ø³Ø§ÛŒØª\"\"\"\n",
    "        queue = deque([self.base_url])\n",
    "        self.visited_urls.add(self.base_url)\n",
    "        \n",
    "        print(f\"Ø´Ø±ÙˆØ¹ Ø®Ø²ÛŒØ¯Ù† Ø§Ø²: {self.base_url}\")\n",
    "        \n",
    "        while queue and len(self.pages_data) < self.max_pages:\n",
    "            current_url = queue.popleft()\n",
    "            print(f\"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ({len(self.pages_data) + 1}/{self.max_pages}): {current_url}\")\n",
    "            \n",
    "            page_data = self.extract_page_content(current_url)\n",
    "            \n",
    "            if page_data:\n",
    "                self.pages_data.append(page_data)\n",
    "                \n",
    "                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ ØµÙ\n",
    "                for link in page_data['links']:\n",
    "                    if link not in self.visited_urls and len(self.visited_urls) < self.max_pages:\n",
    "                        self.visited_urls.add(link)\n",
    "                        queue.append(link)\n",
    "            \n",
    "            time.sleep(1)  # ØªØ§Ø®ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ÙØ´Ø§Ø± Ø¨Ù‡ Ø³Ø±ÙˆØ±\n",
    "        \n",
    "        print(f\"\\nØ®Ø²ÛŒØ¯Ù† ØªÙ…Ø§Ù… Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(self.pages_data)}\")\n",
    "        return self.pages_data\n",
    "    \n",
    "    def save_to_file(self, filename='website_data.json'):\n",
    "        \"\"\"Ø°Ø®ÛŒØ±Ù‡ JSON Ø¯Ø± ÙÙˆÙ„Ø¯Ø± Ø§Ø®ØªØµØ§ØµÛŒ Ø³Ø§ÛŒØª\"\"\"\n",
    "        filepath = os.path.join(self.output_folder, filename)\n",
    "        data = {\n",
    "            'base_url': self.base_url,\n",
    "            'total_pages': len(self.pages_data),\n",
    "            'total_urls_discovered': len(self.visited_urls),\n",
    "            'crawled_at': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'pages': self.pages_data\n",
    "        }\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:\\n {filepath}\")\n",
    "    \n",
    "    def create_sitemap(self, filename='sitemap.txt'):\n",
    "        \"\"\"Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª\"\"\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            for page in self.pages_data:\n",
    "                f.write(f\"{page['url']}\\n\")\n",
    "        print(f\"Ù†Ù‚Ø´Ù‡ Ø³Ø§ÛŒØª Ø¯Ø± ÙØ§ÛŒÙ„ {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯\")\n",
    "\n",
    "\n",
    "# Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡\n",
    "if __name__ == \"__main__\":\n",
    "    # Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯\n",
    "    website_url = \"https://console.melipayamak.com/\"\n",
    "    \n",
    "    # Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø§Ù„Ø±\n",
    "    crawler = WebsiteCrawler(website_url, max_pages=100)\n",
    "    \n",
    "    # Ø´Ø±ÙˆØ¹ Ø®Ø²ÛŒØ¯Ù†\n",
    "    crawler.crawl()\n",
    "    \n",
    "    # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª\n",
    "    crawler.save_to_file('website_data.json')\n",
    "    crawler.create_sitemap('sitemap.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from typing import List, Dict\n",
    "\n",
    "class WebsiteQASystem:\n",
    "    def __init__(self, data_file: str, api_key: str):\n",
    "        \"\"\"\n",
    "        Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ÛŒ ÙˆØ¨Ø³Ø§ÛŒØª\n",
    "        \n",
    "        Args:\n",
    "            data_file: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ JSON Ø­Ø§ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨Ø³Ø§ÛŒØª\n",
    "            api_key: Ú©Ù„ÛŒØ¯ API Ø§Ø² openrouter.ai\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.model = \"openai/gpt-3.5-turbo\"\n",
    "        \n",
    "        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨Ø³Ø§ÛŒØª\n",
    "        with open(data_file, 'r', encoding='utf-8') as f:\n",
    "            self.website_data = json.load(f)\n",
    "        \n",
    "        print(f\"âœ“ {self.website_data['total_pages']} ØµÙØ­Ù‡ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯\")\n",
    "    \n",
    "    def create_context(self, query: str, max_pages: int = 5) -> str:\n",
    "        \"\"\"Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø§Ù†ØªÚ©Ø³Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±\"\"\"\n",
    "        # Ø¬Ø³ØªØ¬ÙˆÛŒ ØµÙØ­Ø§Øª Ù…Ø±ØªØ¨Ø·\n",
    "        relevant_pages = []\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        for page in self.website_data['pages']:\n",
    "            # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ ØµÙØ­Ø§Øª\n",
    "            score = 0\n",
    "            content_lower = (page['title'] + ' ' + \n",
    "                           page['meta_description'] + ' ' + \n",
    "                           page['content']).lower()\n",
    "            \n",
    "            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ\n",
    "            for word in query_lower.split():\n",
    "                if len(word) > 2:\n",
    "                    score += content_lower.count(word)\n",
    "            \n",
    "            if score > 0:\n",
    "                relevant_pages.append((score, page))\n",
    "        \n",
    "        # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† ØµÙØ­Ø§Øª\n",
    "        relevant_pages.sort(reverse=True, key=lambda x: x[0])\n",
    "        top_pages = [page for _, page in relevant_pages[:max_pages]]\n",
    "        \n",
    "        # Ø³Ø§Ø®Øª Ú©Ø§Ù†ØªÚ©Ø³Øª\n",
    "        context = f\"Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨Ø³Ø§ÛŒØª {self.website_data['base_url']}:\\n\\n\"\n",
    "        \n",
    "        for i, page in enumerate(top_pages, 1):\n",
    "            context += f\"=== ØµÙØ­Ù‡ {i}: {page['title']} ===\\n\"\n",
    "            context += f\"URL: {page['url']}\\n\"\n",
    "            if page['meta_description']:\n",
    "                context += f\"ØªÙˆØ¶ÛŒØ­Ø§Øª: {page['meta_description']}\\n\"\n",
    "            context += f\"Ù…Ø­ØªÙˆØ§: {page['content'][:1000]}...\\n\\n\"\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def ask(self, question: str) -> Dict:\n",
    "        \"\"\"Ù¾Ø±Ø³ÛŒØ¯Ù† Ø³ÙˆØ§Ù„ Ø§Ø² Ø³ÛŒØ³ØªÙ…\"\"\"\n",
    "        # Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø§Ù†ØªÚ©Ø³Øª\n",
    "        context = self.create_context(question)\n",
    "        \n",
    "        # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù… Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": f\"\"\"ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¯Ø±Ø¨Ø§Ø±Ù‡ ÛŒÚ© ÙˆØ¨Ø³Ø§ÛŒØª Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ.\n",
    "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø²ÛŒØ± Ø§Ø² ÙˆØ¨Ø³Ø§ÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø³Øª. ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡.\n",
    "Ø§Ú¯Ø± Ø¬ÙˆØ§Ø¨ÛŒ Ø¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù†Ø¨ÙˆØ¯ØŒ Ø¨Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø¨Ú¯Ùˆ Ú©Ù‡ Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.\n",
    "Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒØª Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚ØŒ Ù…ÙÛŒØ¯ Ùˆ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§Ø´Ø¯.\n",
    "\n",
    "{context}\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Ø§Ø±Ø³Ø§Ù„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨Ù‡ API\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"model\": self.model,\n",
    "                \"messages\": messages,\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 1000\n",
    "            }\n",
    "            \n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                headers=headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            answer = result['choices'][0]['message']['content']\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"answer\": answer,\n",
    "                \"model\": self.model\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def interactive_mode(self):\n",
    "        \"\"\"Ø­Ø§Ù„Øª ØªØ¹Ø§Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø® ÙˆØ¨Ø³Ø§ÛŒØª\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"ÙˆØ¨Ø³Ø§ÛŒØª: {self.website_data['base_url']}\")\n",
    "        print(f\"ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {self.website_data['total_pages']}\")\n",
    "        print(\"\\nØ¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ 'exit' ÛŒØ§ 'Ø®Ø±ÙˆØ¬' Ø±Ø§ ØªØ§ÛŒÙ¾ Ú©Ù†ÛŒØ¯\\n\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                question = input(\"\\nâ“ Ø³ÙˆØ§Ù„ Ø´Ù…Ø§: \").strip()\n",
    "                \n",
    "                if question.lower() in ['exit', 'Ø®Ø±ÙˆØ¬', 'quit']:\n",
    "                    print(\"Ø®Ø¯Ø§Ø­Ø§ÙØ¸!\")\n",
    "                    break\n",
    "                \n",
    "                if not question:\n",
    "                    continue\n",
    "                \n",
    "                print(\"\\nâ³ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´...\")\n",
    "                result = self.ask(question)\n",
    "                \n",
    "                if result['success']:\n",
    "                    print(f\"\\nâœ… Ù¾Ø§Ø³Ø®:\\n{result['answer']}\")\n",
    "                else:\n",
    "                    print(f\"\\nâŒ Ø®Ø·Ø§: {result['error']}\")\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\nØ®Ø¯Ø§Ø­Ø§ÙØ¸!\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\nâŒ Ø®Ø·Ø§: {str(e)}\")\n",
    "\n",
    "\n",
    "# Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡\n",
    "if __name__ == \"__main__\":\n",
    "    # Ú©Ù„ÛŒØ¯ API Ø®ÙˆØ¯ Ø±Ø§ Ø§Ø² openrouter.ai Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯\n",
    "    API_KEY = \"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\"\n",
    "    \n",
    "    # Ø§ÛŒØ¬Ø§Ø¯ Ø³ÛŒØ³ØªÙ… Ù¾Ø±Ø³Ø´ Ùˆ Ù¾Ø§Ø³Ø®\n",
    "    qa_system = WebsiteQASystem(\n",
    "        data_file='website_data.json',\n",
    "        api_key=API_KEY\n",
    "    )\n",
    "    \n",
    "    # Ø­Ø§Ù„Øª ØªØ¹Ø§Ù…Ù„ÛŒ\n",
    "    qa_system.interactive_mode()\n",
    "    \n",
    "    # ÛŒØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ…:\n",
    "    # result = qa_system.ask(\"Ù‚ÛŒÙ…Øª Ú¯ÙˆØ´ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ù…Ø³ÙˆÙ†Ú¯ Ú†Ù‚Ø¯Ø± Ø§Ø³ØªØŸ\")\n",
    "    # print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea0d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a575ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa63e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai langchain faiss-cpu tiktoken\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªÙˆ â”€â”€â”€â”€â”€â”€\n",
    "OPENROUTER_API_KEY = \"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\"          # â† Ø§ÛŒÙ†Ø¬Ø§ Ú©Ù„ÛŒØ¯Øª Ø±Ùˆ Ø¨Ø°Ø§Ø±\n",
    "SITE_DATA_JSON     = \"site_data.json\"        # ÙØ§ÛŒÙ„ JSON ØµÙØ­Ø§Øª\n",
    "FAISS_INDEX_FOLDER = \"site_index\"            # ÙÙˆÙ„Ø¯Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# 1. Ø³Ø§Ø®Øª Ú©Ù„Ø§ÛŒÙ†Øª OpenRouter\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    ")\n",
    "\n",
    "# 2. Ù„ÙˆØ¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø§ÛŒØª\n",
    "def load_site_data():\n",
    "    with open(SITE_DATA_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        pages = json.load(f)\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    docs = []\n",
    "    for page in pages:\n",
    "        chunks = splitter.split_text(page['content'])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            docs.append(Document(\n",
    "                page_content=chunk,\n",
    "                metadata={\"url\": page['url'], \"title\": page.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}\n",
    "            ))\n",
    "    return docs\n",
    "\n",
    "# 3. Ø³Ø§Ø®Øª ÛŒØ§ Ù„ÙˆØ¯ Vector Store\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "if not FAISS.index_exists(FAISS_INDEX_FOLDER):\n",
    "    print(\"Ø¯Ø± Ø­Ø§Ù„ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¬Ø¯ÛŒØ¯...\")\n",
    "    docs = load_site_data()\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    vectorstore.save_local(FAISS_INDEX_FOLDER)\n",
    "else:\n",
    "    print(\"Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù‚Ø¨Ù„ÛŒ Ù„ÙˆØ¯ Ø´Ø¯.\")\n",
    "    vectorstore = FAISS.load_local(FAISS_INDEX_FOLDER, embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# 4. ØªØ§Ø¨Ø¹ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨\n",
    "def ask(question: str):\n",
    "    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù‚Ø·Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([f\"Ù…Ù†Ø¨Ø¹: {d.metadata['url']}\\n{d.page_content}\" for d in relevant_docs])\n",
    "\n",
    "    # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ OpenRouter\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"openai/gpt-3.5-turbo\",        # ÛŒØ§ google/gemini-pro ÛŒØ§ anthropic/claude-3-haiku\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªÙ† Ø²ÛŒØ± Ø¬ÙˆØ§Ø¨ Ø¨Ø¯Ù‡ Ùˆ Ø¯Ø± Ø¢Ø®Ø± URL Ù…Ù†Ø¨Ø¹ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†.\"},\n",
    "            {\"role\": \"user\",   \"content\": f\"Ù…ØªÙ†:\\n{context}\\n\\nØ³ÙˆØ§Ù„: {question}\"}\n",
    "        ],\n",
    "        temperature=0.3,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "\n",
    "    answer = completion.choices[0].message.content\n",
    "    sources = [d.metadata['url'] for d in relevant_docs]\n",
    "    return answer, sources\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€ ØªØ³Øª Ø³Ø±ÛŒØ¹ â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        q = input(\"\\nØ³ÙˆØ§Ù„Øª Ú†ÛŒÙ‡ØŸ (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ Ø¨Ù†ÙˆÛŒØ³ exit) \")\n",
    "        if q.lower() in [\"exit\", \"quit\", \"Ø®Ø±ÙˆØ¬\"]:\n",
    "            break\n",
    "        answer, urls = ask(q)\n",
    "        print(\"\\nÙ¾Ø§Ø³Ø®:\")\n",
    "        print(answer)\n",
    "        print(\"\\nÙ…Ù†Ø§Ø¨Ø¹:\")\n",
    "        for u in set(urls):\n",
    "            print(\"  â€¢\", u)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
